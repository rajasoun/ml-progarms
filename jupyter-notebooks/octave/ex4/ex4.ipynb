{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Online Class - Exercise 4 Neural Network Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:06:02.800485Z",
     "start_time": "2018-08-20T07:06:02.715132Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "function checkNNGradients(lambda)\n",
    "%CHECKNNGRADIENTS Creates a small neural network to check the\n",
    "%backpropagation gradients\n",
    "%   CHECKNNGRADIENTS(lambda) Creates a small neural network to check the\n",
    "%   backpropagation gradients, it will output the analytical gradients\n",
    "%   produced by your backprop code and the numerical gradients (computed\n",
    "%   using computeNumericalGradient). These two gradient computations should\n",
    "%   result in very similar values.\n",
    "%\n",
    "\n",
    "if ~exist('lambda', 'var') || isempty(lambda)\n",
    "    lambda = 0;\n",
    "end\n",
    "\n",
    "input_layer_size = 3;\n",
    "hidden_layer_size = 5;\n",
    "num_labels = 3;\n",
    "m = 5;\n",
    "\n",
    "% We generate some 'random' test data\n",
    "Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size);\n",
    "Theta2 = debugInitializeWeights(num_labels, hidden_layer_size);\n",
    "% Reusing debugInitializeWeights to generate X\n",
    "X  = debugInitializeWeights(m, input_layer_size - 1);\n",
    "y  = 1 + mod(1:m, num_labels)';\n",
    "\n",
    "% Unroll parameters\n",
    "nn_params = [Theta1(:) ; Theta2(:)];\n",
    "\n",
    "% Short hand for cost function\n",
    "costFunc = @(p) nnCostFunction(p, input_layer_size, hidden_layer_size, ...\n",
    "                               num_labels, X, y, lambda);\n",
    "\n",
    "[cost, grad] = costFunc(nn_params);\n",
    "numgrad = computeNumericalGradient(costFunc, nn_params);\n",
    "\n",
    "% Visually examine the two gradient computations.  The two columns\n",
    "% you get should be very similar. \n",
    "disp([numgrad grad]);\n",
    "fprintf(['The above two columns you get should be very similar.\\n' ...\n",
    "         '(Left-Your Numerical Gradient, Right-Analytical Gradient)\\n\\n']);\n",
    "\n",
    "% Evaluate the norm of the difference between two solutions.  \n",
    "% If you have a correct implementation, and assuming you used EPSILON = 0.0001 \n",
    "% in computeNumericalGradient.m, then diff below should be less than 1e-9\n",
    "diff = norm(numgrad-grad)/norm(numgrad+grad);\n",
    "\n",
    "fprintf(['If your backpropagation implementation is correct, then \\n' ...\n",
    "         'the relative difference will be small (less than 1e-9). \\n' ...\n",
    "         '\\nRelative Difference: %g\\n'], diff);\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:06:13.075036Z",
     "start_time": "2018-08-20T07:06:13.021347Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "function numgrad = computeNumericalGradient(J, theta)\n",
    "%COMPUTENUMERICALGRADIENT Computes the gradient using \"finite differences\"\n",
    "%and gives us a numerical estimate of the gradient.\n",
    "%   numgrad = COMPUTENUMERICALGRADIENT(J, theta) computes the numerical\n",
    "%   gradient of the function J around theta. Calling y = J(theta) should\n",
    "%   return the function value at theta.\n",
    "\n",
    "% Notes: The following code implements numerical gradient checking, and \n",
    "%        returns the numerical gradient.It sets numgrad(i) to (a numerical \n",
    "%        approximation of) the partial derivative of J with respect to the \n",
    "%        i-th input argument, evaluated at theta. (i.e., numgrad(i) should \n",
    "%        be the (approximately) the partial derivative of J with respect \n",
    "%        to theta(i).)\n",
    "%                \n",
    "\n",
    "numgrad = zeros(size(theta));\n",
    "perturb = zeros(size(theta));\n",
    "e = 1e-4;\n",
    "for p = 1:numel(theta)\n",
    "    % Set perturbation vector\n",
    "    perturb(p) = e;\n",
    "    loss1 = J(theta - perturb);\n",
    "    loss2 = J(theta + perturb);\n",
    "    % Compute Numerical Gradient\n",
    "    numgrad(p) = (loss2 - loss1) / (2*e);\n",
    "    perturb(p) = 0;\n",
    "end\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:06:14.172343Z",
     "start_time": "2018-08-20T07:06:14.133211Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "function W = debugInitializeWeights(fan_out, fan_in)\n",
    "%DEBUGINITIALIZEWEIGHTS Initialize the weights of a layer with fan_in\n",
    "%incoming connections and fan_out outgoing connections using a fixed\n",
    "%strategy, this will help you later in debugging\n",
    "%   W = DEBUGINITIALIZEWEIGHTS(fan_in, fan_out) initializes the weights \n",
    "%   of a layer with fan_in incoming connections and fan_out outgoing \n",
    "%   connections using a fix set of values\n",
    "%\n",
    "%   Note that W should be set to a matrix of size(1 + fan_in, fan_out) as\n",
    "%   the first row of W handles the \"bias\" terms\n",
    "%\n",
    "\n",
    "% Set W to zeros\n",
    "W = zeros(fan_out, 1 + fan_in);\n",
    "\n",
    "% Initialize W using \"sin\", this ensures that W is always of the same\n",
    "% values and will be useful for debugging\n",
    "W = reshape(sin(1:numel(W)), size(W)) / 10;\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:06:17.071196Z",
     "start_time": "2018-08-20T07:06:16.976877Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "function [h, display_array] = displayData(X, example_width)\n",
    "%DISPLAYDATA Display 2D data in a nice grid\n",
    "%   [h, display_array] = DISPLAYDATA(X, example_width) displays 2D data\n",
    "%   stored in X in a nice grid. It returns the figure handle h and the \n",
    "%   displayed array if requested.\n",
    "\n",
    "% Set example_width automatically if not passed in\n",
    "if ~exist('example_width', 'var') || isempty(example_width) \n",
    "\texample_width = round(sqrt(size(X, 2)));\n",
    "end\n",
    "\n",
    "% Gray Image\n",
    "colormap(gray);\n",
    "\n",
    "% Compute rows, cols\n",
    "[m n] = size(X);\n",
    "example_height = (n / example_width);\n",
    "\n",
    "% Compute number of items to display\n",
    "display_rows = floor(sqrt(m));\n",
    "display_cols = ceil(m / display_rows);\n",
    "\n",
    "% Between images padding\n",
    "pad = 1;\n",
    "\n",
    "% Setup blank display\n",
    "display_array = - ones(pad + display_rows * (example_height + pad), ...\n",
    "                       pad + display_cols * (example_width + pad));\n",
    "\n",
    "% Copy each example into a patch on the display array\n",
    "curr_ex = 1;\n",
    "for j = 1:display_rows\n",
    "    for i = 1:display_cols\n",
    "        if curr_ex > m, \n",
    "            break; \n",
    "        end\n",
    "        % Copy the patch\n",
    "\n",
    "        % Get the max value of the patch\n",
    "        max_val = max(abs(X(curr_ex, :)));\n",
    "        display_array(pad + (j - 1) * (example_height + pad) + (1:example_height), ...\n",
    "                      pad + (i - 1) * (example_width + pad) + (1:example_width)) = ...\n",
    "                        reshape(X(curr_ex, :), example_height, example_width) / max_val;\n",
    "        curr_ex = curr_ex + 1;\n",
    "    end\n",
    "    if curr_ex > m, \n",
    "        break; \n",
    "    end\n",
    "end\n",
    "\n",
    "% Display Image\n",
    "h = imagesc(display_array, [-1 1]);\n",
    "\n",
    "% Do not show axis\n",
    "axis image off\n",
    "\n",
    "drawnow;\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:07:37.912563Z",
     "start_time": "2018-08-20T07:07:37.586636Z"
    },
    "code_folding": [
     82,
     93,
     146
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "function [X, fX, i] = fmincg(f, X, options, P1, P2, P3, P4, P5)\n",
    "% Minimize a continuous differentialble multivariate function. Starting point\n",
    "% is given by \"X\" (D by 1), and the function named in the string \"f\", must\n",
    "% return a function value and a vector of partial derivatives. The Polack-\n",
    "% Ribiere flavour of conjugate gradients is used to compute search directions,\n",
    "% and a line search using quadratic and cubic polynomial approximations and the\n",
    "% Wolfe-Powell stopping criteria is used together with the slope ratio method\n",
    "% for guessing initial step sizes. Additionally a bunch of checks are made to\n",
    "% make sure that exploration is taking place and that extrapolation will not\n",
    "% be unboundedly large. The \"length\" gives the length of the run: if it is\n",
    "% positive, it gives the maximum number of line searches, if negative its\n",
    "% absolute gives the maximum allowed number of function evaluations. You can\n",
    "% (optionally) give \"length\" a second component, which will indicate the\n",
    "% reduction in function value to be expected in the first line-search (defaults\n",
    "% to 1.0). The function returns when either its length is up, or if no further\n",
    "% progress can be made (ie, we are at a minimum, or so close that due to\n",
    "% numerical problems, we cannot get any closer). If the function terminates\n",
    "% within a few iterations, it could be an indication that the function value\n",
    "% and derivatives are not consistent (ie, there may be a bug in the\n",
    "% implementation of your \"f\" function). The function returns the found\n",
    "% solution \"X\", a vector of function values \"fX\" indicating the progress made\n",
    "% and \"i\" the number of iterations (line searches or function evaluations,\n",
    "% depending on the sign of \"length\") used.\n",
    "%\n",
    "% Usage: [X, fX, i] = fmincg(f, X, options, P1, P2, P3, P4, P5)\n",
    "%\n",
    "% See also: checkgrad \n",
    "%\n",
    "% Copyright (C) 2001 and 2002 by Carl Edward Rasmussen. Date 2002-02-13\n",
    "%\n",
    "%\n",
    "% (C) Copyright 1999, 2000 & 2001, Carl Edward Rasmussen\n",
    "% \n",
    "% Permission is granted for anyone to copy, use, or modify these\n",
    "% programs and accompanying documents for purposes of research or\n",
    "% education, provided this copyright notice is retained, and note is\n",
    "% made of any changes that have been made.\n",
    "% \n",
    "% These programs and documents are distributed without any warranty,\n",
    "% express or implied.  As the programs were written for research\n",
    "% purposes only, they have not been tested to the degree that would be\n",
    "% advisable in any important application.  All use of these programs is\n",
    "% entirely at the user's own risk.\n",
    "%\n",
    "% [ml-class] Changes Made:\n",
    "% 1) Function name and argument specifications\n",
    "% 2) Output display\n",
    "%\n",
    "\n",
    "% Read options\n",
    "if exist('options', 'var') && ~isempty(options) && isfield(options, 'MaxIter')\n",
    "    length = options.MaxIter;\n",
    "else\n",
    "    length = 100;\n",
    "end\n",
    "\n",
    "\n",
    "RHO = 0.01;                            % a bunch of constants for line searches\n",
    "SIG = 0.5;       % RHO and SIG are the constants in the Wolfe-Powell conditions\n",
    "INT = 0.1;    % don't reevaluate within 0.1 of the limit of the current bracket\n",
    "EXT = 3.0;                    % extrapolate maximum 3 times the current bracket\n",
    "MAX = 20;                         % max 20 function evaluations per line search\n",
    "RATIO = 100;                                      % maximum allowed slope ratio\n",
    "\n",
    "argstr = ['feval(f, X'];                      % compose string used to call function\n",
    "for i = 1:(nargin - 3)\n",
    "  argstr = [argstr, ',P', int2str(i)];\n",
    "end\n",
    "argstr = [argstr, ')'];\n",
    "\n",
    "if max(size(length)) == 2, red=length(2); length=length(1); else red=1; end\n",
    "S=['Iteration '];\n",
    "\n",
    "i = 0;                                            % zero the run length counter\n",
    "ls_failed = 0;                             % no previous line search has failed\n",
    "fX = [];\n",
    "[f1 df1] = eval(argstr);                      % get function value and gradient\n",
    "i = i + (length<0);                                            % count epochs?!\n",
    "s = -df1;                                        % search direction is steepest\n",
    "d1 = -s'*s;                                                 % this is the slope\n",
    "z1 = red/(1-d1);                                  % initial step is red/(|s|+1)\n",
    "\n",
    "while i < abs(length)                                      % while not finished\n",
    "  i = i + (length>0);                                      % count iterations?!\n",
    "\n",
    "  X0 = X; f0 = f1; df0 = df1;                   % make a copy of current values\n",
    "  X = X + z1*s;                                             % begin line search\n",
    "  [f2 df2] = eval(argstr);\n",
    "  i = i + (length<0);                                          % count epochs?!\n",
    "  d2 = df2'*s;\n",
    "  f3 = f1; d3 = d1; z3 = -z1;             % initialize point 3 equal to point 1\n",
    "  if length>0, M = MAX; else M = min(MAX, -length-i); end\n",
    "  success = 0; limit = -1;                     % initialize quanteties\n",
    "  while 1\n",
    "    while ((f2 > f1+z1*RHO*d1) || (d2 > -SIG*d1)) && (M > 0) \n",
    "      limit = z1;                                         % tighten the bracket\n",
    "      if f2 > f1\n",
    "        z2 = z3 - (0.5*d3*z3*z3)/(d3*z3+f2-f3);                 % quadratic fit\n",
    "      else\n",
    "        A = 6*(f2-f3)/z3+3*(d2+d3);                                 % cubic fit\n",
    "        B = 3*(f3-f2)-z3*(d3+2*d2);\n",
    "        z2 = (sqrt(B*B-A*d2*z3*z3)-B)/A;       % numerical error possible - ok!\n",
    "      end\n",
    "      if isnan(z2) || isinf(z2)\n",
    "        z2 = z3/2;                  % if we had a numerical problem then bisect\n",
    "      end\n",
    "      z2 = max(min(z2, INT*z3),(1-INT)*z3);  % don't accept too close to limits\n",
    "      z1 = z1 + z2;                                           % update the step\n",
    "      X = X + z2*s;\n",
    "      [f2 df2] = eval(argstr);\n",
    "      M = M - 1; i = i + (length<0);                           % count epochs?!\n",
    "      d2 = df2'*s;\n",
    "      z3 = z3-z2;                    % z3 is now relative to the location of z2\n",
    "    end\n",
    "    if f2 > f1+z1*RHO*d1 || d2 > -SIG*d1\n",
    "      break;                                                % this is a failure\n",
    "    elseif d2 > SIG*d1\n",
    "      success = 1; break;                                             % success\n",
    "    elseif M == 0\n",
    "      break;                                                          % failure\n",
    "    end\n",
    "    A = 6*(f2-f3)/z3+3*(d2+d3);                      % make cubic extrapolation\n",
    "    B = 3*(f3-f2)-z3*(d3+2*d2);\n",
    "    z2 = -d2*z3*z3/(B+sqrt(B*B-A*d2*z3*z3));        % num. error possible - ok!\n",
    "    if ~isreal(z2) || isnan(z2) || isinf(z2) || z2 < 0 % num prob or wrong sign?\n",
    "      if limit < -0.5                               % if we have no upper limit\n",
    "        z2 = z1 * (EXT-1);                 % the extrapolate the maximum amount\n",
    "      else\n",
    "        z2 = (limit-z1)/2;                                   % otherwise bisect\n",
    "      end\n",
    "    elseif (limit > -0.5) && (z2+z1 > limit)         % extraplation beyond max?\n",
    "      z2 = (limit-z1)/2;                                               % bisect\n",
    "    elseif (limit < -0.5) && (z2+z1 > z1*EXT)       % extrapolation beyond limit\n",
    "      z2 = z1*(EXT-1.0);                           % set to extrapolation limit\n",
    "    elseif z2 < -z3*INT\n",
    "      z2 = -z3*INT;\n",
    "    elseif (limit > -0.5) && (z2 < (limit-z1)*(1.0-INT))  % too close to limit?\n",
    "      z2 = (limit-z1)*(1.0-INT);\n",
    "    end\n",
    "    f3 = f2; d3 = d2; z3 = -z2;                  % set point 3 equal to point 2\n",
    "    z1 = z1 + z2; X = X + z2*s;                      % update current estimates\n",
    "    [f2 df2] = eval(argstr);\n",
    "    M = M - 1; i = i + (length<0);                             % count epochs?!\n",
    "    d2 = df2'*s;\n",
    "  end                                                      % end of line search\n",
    "\n",
    "  if success                                         % if line search succeeded\n",
    "    f1 = f2; fX = [fX' f1]';\n",
    "    fprintf('%s %4i | Cost: %4.6e\\r', S, i, f1);\n",
    "    s = (df2'*df2-df1'*df2)/(df1'*df1)*s - df2;      % Polack-Ribiere direction\n",
    "    tmp = df1; df1 = df2; df2 = tmp;                         % swap derivatives\n",
    "    d2 = df1'*s;\n",
    "    if d2 > 0                                      % new slope must be negative\n",
    "      s = -df1;                              % otherwise use steepest direction\n",
    "      d2 = -s'*s;    \n",
    "    end\n",
    "    z1 = z1 * min(RATIO, d1/(d2-realmin));          % slope ratio but max RATIO\n",
    "    d1 = d2;\n",
    "    ls_failed = 0;                              % this line search did not fail\n",
    "  else\n",
    "    X = X0; f1 = f0; df1 = df0;  % restore point from before failed line search\n",
    "    if ls_failed || i > abs(length)          % line search failed twice in a row\n",
    "      break;                             % or we ran out of time, so we give up\n",
    "    end\n",
    "    tmp = df1; df1 = df2; df2 = tmp;                         % swap derivatives\n",
    "    s = -df1;                                                    % try steepest\n",
    "    d1 = -s'*s;\n",
    "    z1 = 1/(1-d1);                     \n",
    "    ls_failed = 1;                                    % this line search failed\n",
    "  end\n",
    "  if exist('OCTAVE_VERSION')\n",
    "    fflush(stdout);\n",
    "  end\n",
    "end\n",
    "fprintf('\\n');\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:07:44.712571Z",
     "start_time": "2018-08-20T07:07:44.521789Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "function [J grad] = nnCostFunction(nn_params, ...\n",
    "                                   input_layer_size, ...\n",
    "                                   hidden_layer_size, ...\n",
    "                                   num_labels, ...\n",
    "                                   X, y, lambda)\n",
    "%NNCOSTFUNCTION Implements the neural network cost function for a two layer\n",
    "%neural network which performs classification\n",
    "%   [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...\n",
    "%   X, y, lambda) computes the cost and gradient of the neural network. The\n",
    "%   parameters for the neural network are \"unrolled\" into the vector\n",
    "%   nn_params and need to be converted back into the weight matrices. \n",
    "% \n",
    "%   The returned parameter grad should be a \"unrolled\" vector of the\n",
    "%   partial derivatives of the neural network.\n",
    "%\n",
    "\n",
    "% Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "% for our 2 layer neural network\n",
    "Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\n",
    "                 hidden_layer_size, (input_layer_size + 1));\n",
    "\n",
    "Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...\n",
    "                 num_labels, (hidden_layer_size + 1));\n",
    "\n",
    "% Setup some useful variables\n",
    "m = size(X, 1);\n",
    "         \n",
    "% You need to return the following variables correctly \n",
    "J = 0;\n",
    "Theta1_grad = zeros(size(Theta1));\n",
    "Theta2_grad = zeros(size(Theta2));\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: You should complete the code by working through the\n",
    "%               following parts.\n",
    "%\n",
    "% Part 1: Feedforward the neural network and return the cost in the\n",
    "%         variable J. After implementing Part 1, you can verify that your\n",
    "%         cost function computation is correct by verifying the cost\n",
    "%         computed in ex4.m\n",
    "%\n",
    "% Part 2: Implement the backpropagation algorithm to compute the gradients\n",
    "%         Theta1_grad and Theta2_grad. You should return the partial derivatives of\n",
    "%         the cost function with respect to Theta1 and Theta2 in Theta1_grad and\n",
    "%         Theta2_grad, respectively. After implementing Part 2, you can check\n",
    "%         that your implementation is correct by running checkNNGradients\n",
    "%\n",
    "%         Note: The vector y passed into the function is a vector of labels\n",
    "%               containing values from 1..K. You need to map this vector into a \n",
    "%               binary vector of 1's and 0's to be used with the neural network\n",
    "%               cost function.\n",
    "%\n",
    "%         Hint: We recommend implementing backpropagation using a for-loop\n",
    "%               over the training examples if you are implementing it for the \n",
    "%               first time.\n",
    "%\n",
    "% Part 3: Implement regularization with the cost function and gradients.\n",
    "%\n",
    "%         Hint: You can implement this around the code for\n",
    "%               backpropagation. That is, you can compute the gradients for\n",
    "%               the regularization separately and then add them to Theta1_grad\n",
    "%               and Theta2_grad from Part 2.\n",
    "%\n",
    "\n",
    "\n",
    "% Convert y to matrix\n",
    "% XXX(SaveTheRbtz@): Curious how it can be vectorized (Should reread ex3 for\n",
    "% logical arrays)\n",
    "number_of_classes = length(unique(y));\n",
    "Y = zeros(number_of_classes, m);\n",
    "for i = 1:m\n",
    "    Y(y(i), i) = 1;\n",
    "endfor\n",
    "\n",
    "% Do forward propagation\n",
    "% Copy/Paste from ex3 predict.m\n",
    "% FIXME(SaveTheRbtz@): Move to separate function\n",
    "A1 = [ones(1, m); X'];\n",
    "Z2=Theta1*A1;\n",
    "A2 = [ones(1, m); sigmoid(Z2)];\n",
    "Z3=Theta2*A2;\n",
    "A3 = sigmoid(Z3);\n",
    "\n",
    "% A3 here is our h0\n",
    "h0 = A3;\n",
    "\n",
    "% Compute cost function\n",
    "% XXX(SaveTheRbtz@): Slightly modified version of ex2 costFunction\n",
    "J = (1/m)*sum(sum(-Y.*log(h0) - (1-Y).*log(1-h0)));\n",
    "\n",
    "% Add some regularization\n",
    "% XXX(SaveTheRbtz@): Also borrowed from ex2 costFunctionReg\n",
    "penalize = sum(sum(Theta1(:, 2:end) .^ 2)) + sum(sum(Theta2(:, 2:end) .^ 2));\n",
    "J = J + (lambda/(2*m)) * penalize;\n",
    "\n",
    "% Implement backpropagation\n",
    "delta_3 = A3 - Y;\n",
    "delta_2 = (Theta2'*delta_3)(2:end, :) .* sigmoidGradient(Z2);\n",
    "\n",
    "% Calculate gradients\n",
    "Theta1_unreg_grad = (delta_2 * A1')/m;\n",
    "Theta2_unreg_grad = (delta_3 * A2')/m;\n",
    "\n",
    "% Regularize\n",
    "Theta1_grad = Theta1_unreg_grad + (lambda/m) * Theta1;\n",
    "Theta2_grad = Theta2_unreg_grad + (lambda/m) * Theta2;\n",
    "\n",
    "Theta1_grad(:, 1) = Theta1_unreg_grad(:, 1);\n",
    "Theta2_grad(:, 1) = Theta2_unreg_grad(:, 1);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "% -------------------------------------------------------------\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "% Unroll gradients\n",
    "grad = [Theta1_grad(:) ; Theta2_grad(:)];\n",
    "\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:07:45.593780Z",
     "start_time": "2018-08-20T07:07:45.561013Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "function p = predict(Theta1, Theta2, X)\n",
    "%PREDICT Predict the label of an input given a trained neural network\n",
    "%   p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the\n",
    "%   trained weights of a neural network (Theta1, Theta2)\n",
    "\n",
    "% Useful values\n",
    "m = size(X, 1);\n",
    "num_labels = size(Theta2, 1);\n",
    "\n",
    "% You need to return the following variables correctly \n",
    "p = zeros(size(X, 1), 1);\n",
    "\n",
    "h1 = sigmoid([ones(m, 1) X] * Theta1');\n",
    "h2 = sigmoid([ones(m, 1) h1] * Theta2');\n",
    "[dummy, p] = max(h2, [], 2);\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:07:46.485871Z",
     "start_time": "2018-08-20T07:07:46.436991Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "function W = randInitializeWeights(L_in, L_out)\n",
    "%RANDINITIALIZEWEIGHTS Randomly initialize the weights of a layer with L_in\n",
    "%incoming connections and L_out outgoing connections\n",
    "%   W = RANDINITIALIZEWEIGHTS(L_in, L_out) randomly initializes the weights \n",
    "%   of a layer with L_in incoming connections and L_out outgoing \n",
    "%   connections. \n",
    "%\n",
    "%   Note that W should be set to a matrix of size(L_out, 1 + L_in) as\n",
    "%   the first column of W handles the \"bias\" terms\n",
    "%\n",
    "\n",
    "% You need to return the following variables correctly \n",
    "W = zeros(L_out, 1 + L_in);\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: Initialize W randomly so that we break the symmetry while\n",
    "%               training the neural network.\n",
    "%\n",
    "% Note: The first column of W corresponds to the parameters for the bias unit\n",
    "%\n",
    "\n",
    "% Randomly initialize the weights to small values\n",
    "epsilon_init = 0.12;\n",
    "W = rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init;\n",
    "\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:07:47.344968Z",
     "start_time": "2018-08-20T07:07:47.330036Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "function g = sigmoid(z)\n",
    "%SIGMOID Compute sigmoid functoon\n",
    "%   J = SIGMOID(z) computes the sigmoid of z.\n",
    "\n",
    "g = 1.0 ./ (1.0 + exp(-z));\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:07:48.267669Z",
     "start_time": "2018-08-20T07:07:48.230008Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "function g = sigmoidGradient(z)\n",
    "%SIGMOIDGRADIENT returns the gradient of the sigmoid function\n",
    "%evaluated at z\n",
    "%   g = SIGMOIDGRADIENT(z) computes the gradient of the sigmoid function\n",
    "%   evaluated at z. This should work regardless if z is a matrix or a\n",
    "%   vector. In particular, if z is a vector or matrix, you should return\n",
    "%   the gradient for each element.\n",
    "\n",
    "g = zeros(size(z));\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: Compute the gradient of the sigmoid function evaluated at\n",
    "%               each value of z (z can be a matrix, vector or scalar).\n",
    "\n",
    "\n",
    "\n",
    "g = sigmoid(z) .* (1 - sigmoid(z));\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "% =============================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:07:49.792308Z",
     "start_time": "2018-08-20T07:07:49.771616Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J\n",
      "\u001b[H\u001b[2J\n"
     ]
    }
   ],
   "source": [
    "clear ; close all; clc\n",
    "\n",
    "%% Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400;  % 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25;   % 25 hidden units\n",
    "num_labels = 10;          % 10 labels, from 1 to 10   \n",
    "                          % (note that we have mapped \"0\" to label 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Part 1: Loading and Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:07:51.751954Z",
     "start_time": "2018-08-20T07:07:51.538968Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and Visualizing Data ...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANMAAADTCAMAAAAs2dbrAAAAwFBMVEUAAAAEBAQICAgMDAwQEBAU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\nvbKf3fkfXvypS3dh+vf9/h+u5c1cMvbAZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% Load Training Data\n",
    "fprintf('Loading and Visualizing Data ...\\n')\n",
    "load('../../data/digits_neural_networks.mat');\n",
    "m = size(X, 1);\n",
    "\n",
    "% Randomly select 100 data points to display\n",
    "sel = randperm(size(X, 1));\n",
    "sel = sel(1:100);\n",
    "\n",
    "displayData(X(sel, :));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Part 2 : Loading Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:07:53.016436Z",
     "start_time": "2018-08-20T07:07:52.992877Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Saved Neural Network Parameters ...\n"
     ]
    }
   ],
   "source": [
    "% In this part of the exercise, we load some pre-initialized \n",
    "% neural network parameters.\n",
    "\n",
    "fprintf('\\nLoading Saved Neural Network Parameters ...\\n')\n",
    "\n",
    "% Load the weights into variables Theta1 and Theta2\n",
    "load('../../data/digits_neural_networks_weights.mat');\n",
    "\n",
    "% Unroll parameters \n",
    "nn_params = [Theta1(:) ; Theta2(:)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Part 3 : Compute Cost (Feedforward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:07:54.502499Z",
     "start_time": "2018-08-20T07:07:54.306871Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feedforward Using Neural Network ...\n",
      "Cost at parameters (loaded from digits_neural_networks_weights): 0.287629 \n",
      "(this value should be about 0.287629)\n"
     ]
    }
   ],
   "source": [
    "%% ================ Part 3: Compute Cost (Feedforward) ================\n",
    "%  To the neural network, you should first start by implementing the\n",
    "%  feedforward part of the neural network that returns the cost only. You\n",
    "%  should complete the code in nnCostFunction.m to return cost. After\n",
    "%  implementing the feedforward to compute the cost, you can verify that\n",
    "%  your implementation is correct by verifying that you get the same cost\n",
    "%  as us for the fixed debugging parameters.\n",
    "%\n",
    "%  We suggest implementing the feedforward cost *without* regularization\n",
    "%  first so that it will be easier for you to debug. Later, in part 4, you\n",
    "%  will get to implement the regularized cost.\n",
    "%\n",
    "fprintf('\\nFeedforward Using Neural Network ...\\n')\n",
    "\n",
    "% Weight regularization parameter (we set this to 0 here).\n",
    "lambda = 0;\n",
    "\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...\n",
    "                   num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['Cost at parameters (loaded from digits_neural_networks_weights): %f '...\n",
    "         '\\n(this value should be about 0.287629)\\n'], J);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Part 4 : Implement Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:07:56.050216Z",
     "start_time": "2018-08-20T07:07:55.887281Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking Cost Function (w/ Regularization) ... \n",
      "Cost at parameters (loaded from digits_neural_networks_weights): 0.383770 \n",
      "(this value should be about 0.383770)\n"
     ]
    }
   ],
   "source": [
    "%% =============== Part 4: Implement Regularization ===============\n",
    "%  Once your cost function implementation is correct, you should now\n",
    "%  continue to implement the regularization with the cost.\n",
    "%\n",
    "\n",
    "fprintf('\\nChecking Cost Function (w/ Regularization) ... \\n')\n",
    "\n",
    "% Weight regularization parameter (we set this to 1 here).\n",
    "lambda = 1;\n",
    "\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...\n",
    "                   num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['Cost at parameters (loaded from digits_neural_networks_weights): %f '...\n",
    "         '\\n(this value should be about 0.383770)\\n'], J);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Part 5 : Sigmoid Gradient  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:07:57.500621Z",
     "start_time": "2018-08-20T07:07:57.470367Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating sigmoid gradient...\n",
      "Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\n",
      "  \n",
      "0.196612 0.235004 0.250000 0.235004 0.196612 \n",
      "\n",
      "\n",
      "  0.196612 0.235004 0.250000 0.235004 0.196612 \n"
     ]
    }
   ],
   "source": [
    "%% ================ Part 5: Sigmoid Gradient  ================\n",
    "%  Before you start implementing the neural network, you will first\n",
    "%  implement the gradient for the sigmoid function. You should complete the\n",
    "%  code in the sigmoidGradient.m file.\n",
    "%\n",
    "\n",
    "fprintf('\\nEvaluating sigmoid gradient...\\n')\n",
    "\n",
    "g = sigmoidGradient([-1 -0.5 0 0.5 1]);\n",
    "fprintf('Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\\n  ');\n",
    "fprintf('%f ', g);\n",
    "fprintf('\\n\\n');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Part 6: Initializing Pameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:07:58.940992Z",
     "start_time": "2018-08-20T07:07:58.913340Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Neural Network Parameters ...\n"
     ]
    }
   ],
   "source": [
    "%% ================ Part 6: Initializing Pameters ================\n",
    "%  In this part of the exercise, you will be starting to implment a two\n",
    "%  layer neural network that classifies digits. You will start by\n",
    "%  implementing a function to initialize the weights of the neural network\n",
    "%  (randInitializeWeights.m)\n",
    "\n",
    "fprintf('\\nInitializing Neural Network Parameters ...\\n')\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);\n",
    "\n",
    "% Unroll parameters\n",
    "initial_nn_params = [initial_Theta1(:) ; initial_Theta2(:)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Part 7: Implement Backpropagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:08:00.551046Z",
     "start_time": "2018-08-20T07:08:00.439061Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking Backpropagation... \n",
      "  -9.2783e-03  -9.2783e-03\n",
      "   8.8991e-03   8.8991e-03\n",
      "  -8.3601e-03  -8.3601e-03\n",
      "   7.6281e-03   7.6281e-03\n",
      "  -6.7480e-03  -6.7480e-03\n",
      "  -3.0498e-06  -3.0498e-06\n",
      "   1.4287e-05   1.4287e-05\n",
      "  -2.5938e-05  -2.5938e-05\n",
      "   3.6988e-05   3.6988e-05\n",
      "  -4.6876e-05  -4.6876e-05\n",
      "  -1.7506e-04  -1.7506e-04\n",
      "   2.3315e-04   2.3315e-04\n",
      "  -2.8747e-04  -2.8747e-04\n",
      "   3.3532e-04   3.3532e-04\n",
      "  -3.7622e-04  -3.7622e-04\n",
      "  -9.6266e-05  -9.6266e-05\n",
      "   1.1798e-04   1.1798e-04\n",
      "  -1.3715e-04  -1.3715e-04\n",
      "   1.5325e-04   1.5325e-04\n",
      "  -1.6656e-04  -1.6656e-04\n",
      "   3.1454e-01   3.1454e-01\n",
      "   1.1106e-01   1.1106e-01\n",
      "   9.7401e-02   9.7401e-02\n",
      "   1.6409e-01   1.6409e-01\n",
      "   5.7574e-02   5.7574e-02\n",
      "   5.0458e-02   5.0458e-02\n",
      "   1.6457e-01   1.6457e-01\n",
      "   5.7787e-02   5.7787e-02\n",
      "   5.0753e-02   5.0753e-02\n",
      "   1.5834e-01   1.5834e-01\n",
      "   5.5924e-02   5.5924e-02\n",
      "   4.9162e-02   4.9162e-02\n",
      "   1.5113e-01   1.5113e-01\n",
      "   5.3697e-02   5.3697e-02\n",
      "   4.7146e-02   4.7146e-02\n",
      "   1.4957e-01   1.4957e-01\n",
      "   5.3154e-02   5.3154e-02\n",
      "   4.6560e-02   4.6560e-02\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "\n",
      "Relative Difference: 2.27415e-11\n"
     ]
    }
   ],
   "source": [
    "%% =============== Part 7: Implement Backpropagation ===============\n",
    "%  Once your cost matches up with ours, you should proceed to implement the\n",
    "%  backpropagation algorithm for the neural network. You should add to the\n",
    "%  code you've written in nnCostFunction.m to return the partial\n",
    "%  derivatives of the parameters.\n",
    "%\n",
    "fprintf('\\nChecking Backpropagation... \\n');\n",
    "\n",
    "%  Check gradients by running checkNNGradients\n",
    "checkNNGradients;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Part 8: Implement Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:08:02.192407Z",
     "start_time": "2018-08-20T07:08:01.923176Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking Backpropagation (w/ Regularization) ... \n",
      "  -9.2783e-03  -9.2783e-03\n",
      "   8.8991e-03   8.8991e-03\n",
      "  -8.3601e-03  -8.3601e-03\n",
      "   7.6281e-03   7.6281e-03\n",
      "  -6.7480e-03  -6.7480e-03\n",
      "  -1.6768e-02  -1.6768e-02\n",
      "   3.9433e-02   3.9433e-02\n",
      "   5.9336e-02   5.9336e-02\n",
      "   2.4764e-02   2.4764e-02\n",
      "  -3.2688e-02  -3.2688e-02\n",
      "  -6.0174e-02  -6.0174e-02\n",
      "  -3.1961e-02  -3.1961e-02\n",
      "   2.4923e-02   2.4923e-02\n",
      "   5.9772e-02   5.9772e-02\n",
      "   3.8641e-02   3.8641e-02\n",
      "  -1.7370e-02  -1.7370e-02\n",
      "  -5.7566e-02  -5.7566e-02\n",
      "  -4.5196e-02  -4.5196e-02\n",
      "   9.1459e-03   9.1459e-03\n",
      "   5.4610e-02   5.4610e-02\n",
      "   3.1454e-01   3.1454e-01\n",
      "   1.1106e-01   1.1106e-01\n",
      "   9.7401e-02   9.7401e-02\n",
      "   1.1868e-01   1.1868e-01\n",
      "   3.8193e-05   3.8193e-05\n",
      "   3.3693e-02   3.3693e-02\n",
      "   2.0399e-01   2.0399e-01\n",
      "   1.1715e-01   1.1715e-01\n",
      "   7.5480e-02   7.5480e-02\n",
      "   1.2570e-01   1.2570e-01\n",
      "  -4.0759e-03  -4.0759e-03\n",
      "   1.6968e-02   1.6968e-02\n",
      "   1.7634e-01   1.7634e-01\n",
      "   1.1313e-01   1.1313e-01\n",
      "   8.6163e-02   8.6163e-02\n",
      "   1.3229e-01   1.3229e-01\n",
      "  -4.5296e-03  -4.5296e-03\n",
      "   1.5005e-03   1.5005e-03\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "\n",
      "Relative Difference: 2.21503e-11\n",
      "\n",
      "\n",
      "Cost at (fixed) debugging parameters (w/ lambda = 3.000000): 0.576051 \n",
      "(for lambda = 3, this value should be about 0.576051)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%% =============== Part 8: Implement Regularization ===============\n",
    "%  Once your backpropagation implementation is correct, you should now\n",
    "%  continue to implement the regularization with the cost and gradient.\n",
    "%\n",
    "\n",
    "fprintf('\\nChecking Backpropagation (w/ Regularization) ... \\n')\n",
    "\n",
    "%  Check gradients by running checkNNGradients\n",
    "lambda = 3;\n",
    "checkNNGradients(lambda);\n",
    "\n",
    "% Also output the costFunction debugging values\n",
    "debug_J  = nnCostFunction(nn_params, input_layer_size, ...\n",
    "                          hidden_layer_size, num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['\\n\\nCost at (fixed) debugging parameters (w/ lambda = %f): %f ' ...\n",
    "         '\\n(for lambda = 3, this value should be about 0.576051)\\n\\n'], lambda, debug_J);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Part 8: Training NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:08:15.138289Z",
     "start_time": "2018-08-20T07:08:04.543117Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Neural Network... \n",
      "Iteration    50 | Cost: 4.572720e-01\n"
     ]
    }
   ],
   "source": [
    "%% =================== Part 8: Training NN ===================\n",
    "%  You have now implemented all the code necessary to train a neural \n",
    "%  network. To train your neural network, we will now use \"fmincg\", which\n",
    "%  is a function which works similarly to \"fminunc\". Recall that these\n",
    "%  advanced optimizers are able to train our cost functions efficiently as\n",
    "%  long as we provide them with the gradient computations.\n",
    "%\n",
    "fprintf('\\nTraining Neural Network... \\n')\n",
    "\n",
    "%  After you have completed the assignment, change the MaxIter to a larger\n",
    "%  value to see how more training helps.\n",
    "options = optimset('MaxIter', 50);\n",
    "\n",
    "%  You should also try different values of lambda\n",
    "lambda = 1;\n",
    "\n",
    "% Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = @(p) nnCostFunction(p, ...\n",
    "                                   input_layer_size, ...\n",
    "                                   hidden_layer_size, ...\n",
    "                                   num_labels, X, y, lambda);\n",
    "\n",
    "% Now, costFunction is a function that takes in only one argument (the\n",
    "% neural network parameters)\n",
    "[nn_params, cost] = fmincg(costFunction, initial_nn_params, options);\n",
    "\n",
    "% Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\n",
    "                 hidden_layer_size, (input_layer_size + 1));\n",
    "\n",
    "Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...\n",
    "                 num_labels, (hidden_layer_size + 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Part 9 : Visualize Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:08:34.381883Z",
     "start_time": "2018-08-20T07:08:34.283506Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualizing Neural Network... \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGoAAABqCAMAAABj/zSlAAAAwFBMVEUAAAAEBAQICAgMDAwQEBAU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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%% ================= Part 9: Visualize Weights =================\n",
    "%  You can now \"visualize\" what the neural network is learning by \n",
    "%  displaying the hidden units to see what features they are capturing in \n",
    "%  the data.\n",
    "\n",
    "fprintf('\\nVisualizing Neural Network... \\n')\n",
    "\n",
    "displayData(Theta1(:, 2:end));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Part 10: Implement Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T07:08:58.049920Z",
     "start_time": "2018-08-20T07:08:58.003167Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Accuracy: 95.820000\n"
     ]
    }
   ],
   "source": [
    "%% ================= Part 10: Implement Predict =================\n",
    "%  After training the neural network, we would like to use it to predict\n",
    "%  the labels. You will now implement the \"predict\" function to use the\n",
    "%  neural network to predict the labels of the training set. This lets\n",
    "%  you compute the training set accuracy.\n",
    "\n",
    "pred = predict(Theta1, Theta2, X);\n",
    "\n",
    "fprintf('\\nTraining Set Accuracy: %f\\n', mean(double(pred == y)) * 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
